{
  "paragraphs": [
    {
      "text": "%md\n### This lab will cover caching and memory tuning. Start by caching the trips and stations RDDs we used previously. Compare the different storage levels including seralized vs not, and replication. Review the slides or documentation for the StorageLevels.",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1426821439297_-1490413757",
      "id": "20150319-231719_2114863791",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch3\u003eThis lab will cover caching and memory tuning. Start by caching the trips and stations RDDs we used previously. Compare the different storage levels including seralized vs not, and replication. Review the slides or documentation for the StorageLevels.\u003c/h3\u003e\n"
      },
      "dateCreated": "Mar 19, 2015 11:17:19 PM",
      "dateStarted": "Mar 24, 2015 10:42:54 AM",
      "dateFinished": "Mar 24, 2015 10:42:56 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "import org.apache.spark.storage._\n\nobject utils extends Serializable {\n    case class Trip(\n      id: Int,\n      duration: Int,\n      startDate: String,\n      startStation: String,\n      startTerminal: Int,\n      endDate: String,\n      endStation: String,\n      endTerminal: Int,\n      bike: Int,\n      subscriberType: String,\n      zipCode: Option[String]\n    )\n    \n    object Trip {\n      def parse(i: Array[String]) \u003d {\n        val zip \u003d i.length match {\n          case 11 \u003d\u003e Some(i(10))\n          case _ \u003d\u003e None\n        }\n        Trip(i(0).toInt, i(1).toInt, i(2), i(3), i(4).toInt, i(5), i(6), i(7).toInt, i(8).toInt, i(9), zip)\n      }\n    }    \n    \n    case class Station(\n      id: Int,\n      name: String,\n      lat: Double,\n      lon: Double,\n      docks: Int,\n      landmark: String,\n      installDate: String\n    )\n    \n    object Station {\n      def parse(i: Array[String]) \u003d {\n        Station(i(0).toInt, i(1), i(2).toDouble, i(3).toDouble, i(4).toInt, i(5), i(6))\n      }\n    }\n}\n\nval input1 \u003d sc.textFile(\"data/trips/*\")\n\nval header1 \u003d input1.first // to skip the header row\n\nval trips \u003d input1.\n filter(_ !\u003d header1).\n map(_.split(\",\")).\n map(utils.Trip.parse(_))\n\nval input2 \u003d sc.textFile(\"data/stations/*\")\n\nval header2 \u003d input2.first // to skip the header row\n\nval stations \u003d input2.\n  filter(_ !\u003d header2).\n  map(_.split(\",\")).\n  map(utils.Station.parse(_))",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "tableHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1426821494220_127895956",
      "id": "20150319-231814_627519224",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "import org.apache.spark.storage._\ndefined module utils\ninput1: org.apache.spark.rdd.RDD[String] \u003d data/trips/* MappedRDD[19] at textFile at \u003cconsole\u003e:26\nheader1: String \u003d Trip ID,Duration,Start Date,Start Station,Start Terminal,End Date,End Station,End Terminal,Bike #,Subscription Type,Zip Code\ntrips: org.apache.spark.rdd.RDD[utils.Trip] \u003d MappedRDD[22] at map at \u003cconsole\u003e:35\ninput2: org.apache.spark.rdd.RDD[String] \u003d data/stations/* MappedRDD[24] at textFile at \u003cconsole\u003e:26\nheader2: String \u003d station_id,name,lat,long,dockcount,landmark,installation,notes\nstations: org.apache.spark.rdd.RDD[utils.Station] \u003d MappedRDD[27] at map at \u003cconsole\u003e:35\n"
      },
      "dateCreated": "Mar 19, 2015 11:18:14 PM",
      "dateStarted": "Apr 12, 2015 2:55:26 PM",
      "dateFinished": "Apr 12, 2015 2:55:28 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n### Let\u0027s start by calculating the average duration of trips both by start AND end terminal in the trips RDD. Refer back to Lab 3 for the transformations to run the calculation.\n\n1. First run the job without any caching\n2. Now try calling `cache` on the trips RDD and re-run the transformation\n3. Finally, use `persist(StorageLevel.MEMORY_ONLY_SER)` and re-run, compare the storage statistics in the Spark UI",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1426884879748_-2105386234",
      "id": "20150320-165439_545049613",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch3\u003eLet\u0027s start by calculating the average duration of trips both by start AND end terminal in the trips RDD. Refer back to Lab 3 for the transformations to run the calculation.\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eFirst run the job without any caching\u003c/li\u003e\n\u003cli\u003eNow try calling \u003ccode\u003ecache\u003c/code\u003e on the trips RDD and re-run the transformation\u003c/li\u003e\n\u003cli\u003eFinally, use \u003ccode\u003epersist(StorageLevel.MEMORY_ONLY_SER)\u003c/code\u003e and re-run, compare the storage statistics in the Spark UI\u003c/li\u003e\n\u003c/ol\u003e\n"
      },
      "dateCreated": "Mar 20, 2015 4:54:39 PM",
      "dateStarted": "Apr 9, 2015 3:28:23 PM",
      "dateFinished": "Apr 9, 2015 3:28:23 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Solution",
      "text": "//2 - trips.persist(StorageLevel.MEMORY_ONLY)\n//3 - trips.persist(StorageLevel.MEMORY_ONLY_SER)\n\nval durationsByStart \u003d trips.keyBy(_.startTerminal).mapValues(_.duration)\nval durationsByEnd \u003d trips.keyBy(_.endTerminal).mapValues(_.duration)\n\nval resultsStart \u003d durationsByStart.aggregateByKey((0, 0))((acc, value) \u003d\u003e (acc._1 + value, acc._2 + 1), (acc1, acc2) \u003d\u003e (acc1._1 + acc2._1, acc1._2 + acc2._2))\nval avgStart \u003d resultsStart.mapValues(i \u003d\u003e i._1 / i._2)\n\nval resultsEnd \u003d durationsByEnd.aggregateByKey((0, 0))((acc, value) \u003d\u003e (acc._1 + value, acc._2 + 1), (acc1, acc2) \u003d\u003e (acc1._1 + acc2._1, acc1._2 + acc2._2))\nval avgEnd \u003d resultsEnd.mapValues(i \u003d\u003e i._1 / i._2)\n\navgStart.collect()\navgEnd.collect()",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "tableHide": true,
        "title": true,
        "editorHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1426821662268_-1463548088",
      "id": "20150319-232102_2031377407",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "res28: trips.type \u003d MappedRDD[22] at map at \u003cconsole\u003e:35\ndurationsByStart: org.apache.spark.rdd.RDD[(Int, Int)] \u003d MappedValuesRDD[29] at mapValues at \u003cconsole\u003e:34\ndurationsByEnd: org.apache.spark.rdd.RDD[(Int, Int)] \u003d MappedValuesRDD[31] at mapValues at \u003cconsole\u003e:33\nresultsStart: org.apache.spark.rdd.RDD[(Int, (Int, Int))] \u003d ShuffledRDD[32] at aggregateByKey at \u003cconsole\u003e:36\navgStart: org.apache.spark.rdd.RDD[(Int, Int)] \u003d MappedValuesRDD[33] at mapValues at \u003cconsole\u003e:37\nresultsEnd: org.apache.spark.rdd.RDD[(Int, (Int, Int))] \u003d ShuffledRDD[34] at aggregateByKey at \u003cconsole\u003e:36\navgEnd: org.apache.spark.rdd.RDD[(Int, Int)] \u003d MappedValuesRDD[35] at mapValues at \u003cconsole\u003e:37\nres33: Array[(Int, Int)] \u003d Array((84,1716), (34,3437), (56,1004), (76,1439), (4,1084), (16,1190), (82,908), (66,1157), (22,2776), (28,1633), (54,1133), (80,1576), (46,2032), (48,1616), (30,1563), (14,2221), (32,2161), (36,5023), (24,7758), (62,620), (64,683), (50,1605), (42,1050), (74,1059), (72,1436), (6,1163), (70,824), (8,1059), (12,1966), (38,4023), (60,1549), (58,1714), (26,2277), (10,1271), (68,970), (2,969), (39,1419), (61,877), (41,1164), (13,1007), (21,1564), (47,1059), (71,1881), (77,935), (55,708), (53,1590), (25,2481), (29,3419), (59,1161), (65,746), (35,6545), (73,1238), (11,2017), (27,1663), (75,1098), (51,880), (33,6192), (37,2472), (57,1016), (23,2149), (45,761), (83,679), (63,802), (69,743), (67,1122), (7,1234), (3,3028), (9,2068), (49,756), (31,1627), (5,833))\nres34: Array[(Int, Int)] \u003d Array((84,2526), (34,5071), (56,920), (76,1441), (4,1120), (16,2383), (82,1144), (66,1256), (22,3217), (28,1784), (54,1057), (80,1153), (46,1355), (48,1672), (30,1509), (14,867), (32,2071), (36,4180), (24,3933), (62,734), (64,680), (50,1306), (42,1023), (74,946), (72,1510), (6,1060), (70,769), (8,1189), (12,2158), (38,3016), (60,1845), (58,1996), (26,906), (10,2132), (68,927), (2,822), (39,1303), (61,796), (41,1172), (13,1042), (21,1430), (47,926), (71,2563), (77,760), (55,709), (53,1544), (25,2837), (29,2752), (59,1986), (65,760), (35,5624), (73,1974), (11,1206), (27,1620), (75,1135), (51,824), (33,4787), (37,3390), (57,859), (23,1327), (45,781), (63,740), (83,744), (69,730), (67,1387), (7,865), (3,2640), (9,1854), (49,657), (31,2360), (5,1892))\n"
      },
      "dateCreated": "Mar 19, 2015 11:21:02 PM",
      "dateStarted": "Apr 12, 2015 2:55:32 PM",
      "dateFinished": "Apr 12, 2015 2:55:38 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Using the fastutil Collection library for optimized data structures",
      "text": "import it.unimi.dsi.fastutil.objects.Reference2ReferenceArrayMap\nimport org.apache.spark.storage.StorageLevel\n\n// the rest of the code is the same\n\nval rdd \u003d sc.parallelize(1 to 10) map { i \u003d\u003e\n//   val pageDwellTimes \u003d new Reference2ReferenceArrayMap[String, Int]() // fastutil version\n  val pageDwellTimes \u003d scala.collection.mutable.Map[String, Int]() // standard Scala collection\n\n  1 to 400 foreach { i \u003d\u003e pageDwellTimes.put(s\"page $i\", 30) }\n  \n  (s\"user $i\", pageDwellTimes)\n}\n\nrdd.count()",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorHide": false,
        "title": true,
        "tableHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1427478812987_1255632613",
      "id": "20150327-135332_1240506022",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "import it.unimi.dsi.fastutil.objects.Reference2ReferenceArrayMap\nimport org.apache.spark.storage.StorageLevel\nrdd: org.apache.spark.rdd.RDD[(String, scala.collection.mutable.Map[String,Int])] \u003d MappedRDD[9] at map at \u003cconsole\u003e:36\nres33: Long \u003d 10\n"
      },
      "dateCreated": "Mar 27, 2015 1:53:32 PM",
      "dateStarted": "Apr 10, 2015 9:56:38 AM",
      "dateFinished": "Apr 10, 2015 9:56:39 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "config": {},
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1428674102924_390304692",
      "id": "20150410-095502_1109226920",
      "dateCreated": "Apr 10, 2015 9:55:02 AM",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "Lab 4",
  "id": "Lab4",
  "config": {
    "looknfeel": "default"
  },
  "info": {}
}